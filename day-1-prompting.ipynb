{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:24.196156Z","iopub.execute_input":"2025-11-22T07:48:24.196451Z","iopub.status.idle":"2025-11-22T07:48:24.527756Z","shell.execute_reply.started":"2025-11-22T07:48:24.196427Z","shell.execute_reply":"2025-11-22T07:48:24.526886Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:24.528927Z","iopub.execute_input":"2025-11-22T07:48:24.529576Z","iopub.status.idle":"2025-11-22T07:48:24.534148Z","shell.execute_reply.started":"2025-11-22T07:48:24.529549Z","shell.execute_reply":"2025-11-22T07:48:24.533093Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:24.535348Z","iopub.execute_input":"2025-11-22T07:48:24.535630Z","iopub.status.idle":"2025-11-22T07:48:32.827601Z","shell.execute_reply.started":"2025-11-22T07:48:24.535603Z","shell.execute_reply":"2025-11-22T07:48:32.826744Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.18.0 requires google-genai<2.0.0,>=1.45.0, but you have google-genai 1.7.0 which is incompatible.\ngoogle-cloud-aiplatform 1.125.0 requires google-genai<2.0.0,>=1.37.0, but you have google-genai 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:32.829693Z","iopub.execute_input":"2025-11-22T07:48:32.830330Z","iopub.status.idle":"2025-11-22T07:48:34.391565Z","shell.execute_reply.started":"2025-11-22T07:48:32.830298Z","shell.execute_reply":"2025-11-22T07:48:34.390793Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\ngenai.models.Models.generate_content = retry.Retry(predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:34.392482Z","iopub.execute_input":"2025-11-22T07:48:34.393023Z","iopub.status.idle":"2025-11-22T07:48:36.668055Z","shell.execute_reply.started":"2025-11-22T07:48:34.392993Z","shell.execute_reply":"2025-11-22T07:48:36.667384Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Gemini Project\")\nprint(secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:36.668878Z","iopub.execute_input":"2025-11-22T07:48:36.669346Z","iopub.status.idle":"2025-11-22T07:48:36.723978Z","shell.execute_reply.started":"2025-11-22T07:48:36.669321Z","shell.execute_reply":"2025-11-22T07:48:36.723223Z"}},"outputs":[{"name":"stdout","text":"AIzaSyDH9IycFuh2KAV5DjnZ-Y8neHxdR5yXbGk\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\nclient = genai.Client(api_key=secret_value_0)\n\nresponse = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=\"Explain AI to me like I'm a kid.\"\n)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:36.724802Z","iopub.execute_input":"2025-11-22T07:48:36.725058Z","iopub.status.idle":"2025-11-22T07:48:40.615965Z","shell.execute_reply.started":"2025-11-22T07:48:36.725025Z","shell.execute_reply":"2025-11-22T07:48:40.615099Z"}},"outputs":[{"name":"stdout","text":"Imagine you have a really, really smart toy. This toy can learn things, just like you!\n\n**Normal toys do exactly what you tell them.** If you press a button, they make a sound. If you wind them up, they walk.\n\n**AI toys are different.** They can learn from seeing you play, reading books, or hearing you talk. The more they learn, the better they get at doing things, even things you didn't specifically teach them!\n\nThink about it like this:\n\n*   **Teaching a dog a trick:** You show the dog what to do, give it treats when it gets it right, and soon it learns. AI is kind of like that, but the treats are like \"points\" and the things they learn are stored in their \"brain\" (which is a computer).\n\n*   **Playing a video game:** If you play a game a lot, you start to learn the rules, figure out the best strategies, and get better at winning. AI can do the same thing, practicing over and over again until it's really good!\n\n**So, what can AI do?**\n\n*   **Help you find things online:** Like when you ask your grown-up to search for a picture of a dinosaur. AI helps the search engine understand what you're looking for.\n*   **Play games with you:** Some video games use AI to make the computer characters smart and challenging.\n*   **Talk to you:** Like the voice assistant on your grown-up's phone that answers your questions.\n*   **Drive cars:** Some cars are learning to drive themselves using AI!\n*   **Recognize your face:** If your grown-up has a phone with face unlock, it uses AI to know it's you.\n\n**Is AI magic?**\n\nNo, it's not magic! It's just a lot of clever programming and data. People are working hard to make AI smarter and more helpful every day.\n\n**Important thing to remember:** AI can be really helpful, but it's important to use it responsibly and make sure it's used for good things. And remember that AI is still learning too, so it doesn't always get things right!\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:40.616848Z","iopub.execute_input":"2025-11-22T07:48:40.617161Z","iopub.status.idle":"2025-11-22T07:48:40.623927Z","shell.execute_reply.started":"2025-11-22T07:48:40.617133Z","shell.execute_reply":"2025-11-22T07:48:40.623030Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Imagine you have a really, really smart toy. This toy can learn things, just like you!\n\n**Normal toys do exactly what you tell them.** If you press a button, they make a sound. If you wind them up, they walk.\n\n**AI toys are different.** They can learn from seeing you play, reading books, or hearing you talk. The more they learn, the better they get at doing things, even things you didn't specifically teach them!\n\nThink about it like this:\n\n*   **Teaching a dog a trick:** You show the dog what to do, give it treats when it gets it right, and soon it learns. AI is kind of like that, but the treats are like \"points\" and the things they learn are stored in their \"brain\" (which is a computer).\n\n*   **Playing a video game:** If you play a game a lot, you start to learn the rules, figure out the best strategies, and get better at winning. AI can do the same thing, practicing over and over again until it's really good!\n\n**So, what can AI do?**\n\n*   **Help you find things online:** Like when you ask your grown-up to search for a picture of a dinosaur. AI helps the search engine understand what you're looking for.\n*   **Play games with you:** Some video games use AI to make the computer characters smart and challenging.\n*   **Talk to you:** Like the voice assistant on your grown-up's phone that answers your questions.\n*   **Drive cars:** Some cars are learning to drive themselves using AI!\n*   **Recognize your face:** If your grown-up has a phone with face unlock, it uses AI to know it's you.\n\n**Is AI magic?**\n\nNo, it's not magic! It's just a lot of clever programming and data. People are working hard to make AI smarter and more helpful every day.\n\n**Important thing to remember:** AI can be really helpful, but it's important to use it responsibly and make sure it's used for good things. And remember that AI is still learning too, so it doesn't always get things right!\n"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"chat = client.chats.create(model ='gemini-2.0-flash', history=[])\nresponse = chat.send_message('Hello! My name is Akmal Fikry.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:40.624749Z","iopub.execute_input":"2025-11-22T07:48:40.625099Z","iopub.status.idle":"2025-11-22T07:48:41.157692Z","shell.execute_reply.started":"2025-11-22T07:48:40.625070Z","shell.execute_reply":"2025-11-22T07:48:41.156780Z"}},"outputs":[{"name":"stdout","text":"Hello Akmal Fikry, it's nice to meet you! How can I help you today?\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"response = chat.send_message('Can youy explain me about the Cricket history')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:41.160643Z","iopub.execute_input":"2025-11-22T07:48:41.160970Z","iopub.status.idle":"2025-11-22T07:48:49.188962Z","shell.execute_reply.started":"2025-11-22T07:48:41.160944Z","shell.execute_reply":"2025-11-22T07:48:49.188091Z"}},"outputs":[{"name":"stdout","text":"Okay, let's delve into the history of cricket! It's a rich and fascinating story spanning centuries. Here's a breakdown:\n\n**Early Origins & Development (16th - 18th Centuries):**\n\n*   **Possible Roots (13th Century):** Some theories suggest early forms of the game existed as far back as the 13th century, possibly as a rural pastime played by children. However, concrete evidence is scarce.\n*   **16th Century (England):** The earliest definitive reference to cricket is in England around the 16th century, primarily in the south-east counties. It's believed to have evolved from earlier bat-and-ball games.\n*   **17th Century: Growing Popularity:** Cricket began to gain popularity as a more organized sport, particularly among the gentry. Gambling on matches became common, fueling its growth.\n*   **18th Century: Formalization and MCC:** The Marylebone Cricket Club (MCC) was founded in 1787 at Lord's Cricket Ground in London. The MCC became the guardian of the Laws of Cricket, standardizing the rules and providing a framework for the game.  Key developments included the standardization of pitch length and the introduction of three stumps.\n\n**The Rise of First-Class Cricket (19th Century):**\n\n*   **County Cricket:** County cricket began to take shape in England, with teams representing different counties competing in organized matches. This became a cornerstone of the domestic game.\n*   **Expansion Globally:** British colonization played a major role in spreading cricket across the globe. The game took root in countries like Australia, India, South Africa, the West Indies, and New Zealand.\n*   **First International Match:** The first officially recognized international cricket match was played between the United States and Canada in 1844.\n*   **Test Cricket is Born (1877):**  Test cricket, the highest standard of the game, began in 1877 with a match between Australia and England in Melbourne. These matches were played over multiple days (initially 4, later 5) and became the ultimate test of skill and endurance.  The Ashes series, between England and Australia, became (and remains) the most celebrated Test rivalry.\n\n**20th Century: Professionalization and Global Dominance:**\n\n*   **Continued Growth of Test Cricket:** Test cricket continued to be the dominant form of the game, with more nations joining the international arena.\n*   **World Wars:** Cricket suffered during both World Wars, but it rebounded afterward.\n*   **Emergence of One-Day Cricket (Late 20th Century):**  One-Day International (ODI) cricket emerged in the 1960s and 70s.  These shorter, faster-paced matches were designed to be more entertaining and accessible to a wider audience.\n*   **First Cricket World Cup (1975):** The first Cricket World Cup was held in England in 1975, marking a significant step in the global popularity of limited-overs cricket.\n*   **Apartheid and South Africa:** South Africa was banned from international cricket for many years due to its apartheid policies. The ban was lifted in 1991 after apartheid was abolished.\n\n**21st Century: T20 Revolution and Commercialization:**\n\n*   **The Rise of Twenty20 (T20):**  Twenty20 (T20) cricket, an even shorter format of the game (around 3 hours per match), exploded in popularity in the early 2000s. It offered a faster, more exciting, and more commercially viable product.\n*   **Indian Premier League (IPL):** The Indian Premier League (IPL), launched in 2008, revolutionized cricket. It brought together top players from around the world in a franchise-based T20 league, generating enormous revenue and attracting huge audiences.  Other T20 leagues have since sprung up around the world.\n*   **Continued Commercialization:** Cricket has become increasingly commercialized, with significant revenue generated through broadcasting rights, sponsorships, and merchandise.\n*   **Focus on Player Development:** There's been a growing emphasis on player development programs at the grassroots level in many countries.\n*   **Debate About Formats:**  The proliferation of different formats (Test, ODI, T20) has led to debates about the future of the game and whether Test cricket can maintain its relevance in the face of the popularity of shorter formats.\n\n**Key Turning Points:**\n\n*   **Foundation of the MCC (1787):**  Established the rules and governance of the game.\n*   **The First Test Match (1877):**  Marked the beginning of international competition at the highest level.\n*   **The First Cricket World Cup (1975):**  Brought limited-overs cricket to a global audience.\n*   **The Launch of the IPL (2008):**  Revolutionized the sport with the T20 format and massive commercial success.\n\n**In summary, cricket's history is one of evolution, adaptation, and globalization. From its humble beginnings in rural England to its current status as a global sport with multiple formats and a huge following, cricket has undergone significant changes while retaining its core principles of skill, strategy, and sportsmanship.**\n\nDo you have any specific aspects of cricket history you'd like to know more about? For example, you might be interested in:\n\n*   The history of a specific country's cricket team (e.g., India, Australia, England).\n*   The development of batting or bowling techniques.\n*   The history of the Cricket World Cup.\n*   The evolution of the Laws of Cricket.\n*   Famous cricket players and their contributions.\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"response = chat.send_message('can you tell me something about the Google')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:49.189967Z","iopub.execute_input":"2025-11-22T07:48:49.190293Z","iopub.status.idle":"2025-11-22T07:48:59.092245Z","shell.execute_reply.started":"2025-11-22T07:48:49.190266Z","shell.execute_reply":"2025-11-22T07:48:59.091482Z"}},"outputs":[{"name":"stdout","text":"Okay, let's talk about Google. Here's an overview of its history, development, and impact:\n\n**Origins (1996-1998):**\n\n*   **The BackRub Project (1996):** Google started as a research project by Larry Page and Sergey Brin, Ph.D. students at Stanford University. They were working on improving search engine technology. Their initial goal was to create a superior way to analyze the relationships between websites.\n*   **PageRank Algorithm:** They developed a new algorithm called PageRank, which analyzed the backlinks pointing to a website to determine its importance and relevance. Websites with more high-quality backlinks were ranked higher.\n*   **\"BackRub\" Renamed to \"Google\" (1997):** The initial name for the search engine was \"BackRub,\" but it was later changed to \"Google,\" a play on the mathematical term \"googol\" (a 1 followed by 100 zeros), representing the vast amount of information they aimed to organize.\n*   **Domain Registration (September 15, 1997):** The domain google.com was registered.\n*   **Early Development:** In the early days, Google operated from Page and Brin's dorm rooms and was largely funded by grants and donations.\n\n**Early Growth and Commercialization (1998-2004):**\n\n*   **Official Launch (1998):** Google was officially launched as a company in September 1998, initially operating out of a garage in Menlo Park, California, rented from Susan Wojcicki (who later became the CEO of YouTube).\n*   **Initial Funding (1998):** They secured their first major funding, an investment of $100,000 from Andy Bechtolsheim, co-founder of Sun Microsystems. This helped them officially incorporate.\n*   **Rapid Growth:** Google quickly gained popularity due to its superior search results compared to existing search engines like Yahoo! and AltaVista.\n*   **Moving to the Googleplex (2003):** As the company grew, it moved to its iconic headquarters in Mountain View, California, known as the Googleplex.\n*   **AdWords (2000):** Google launched AdWords (now Google Ads), its advertising program based on pay-per-click (PPC) advertising. This became a major source of revenue for the company.\n*   **IPO (2004):** Google went public with an initial public offering (IPO) in August 2004. The IPO was highly anticipated and generated significant wealth for its founders and early investors.\n\n**Expansion and Diversification (2004-Present):**\n\n*   **Acquisitions:** Google began a series of strategic acquisitions to expand its product offerings and market reach. Key acquisitions included:\n    *   **YouTube (2006):** The video-sharing platform.\n    *   **Android (2005):** The mobile operating system.\n    *   **DoubleClick (2007):** An online advertising company.\n    *   **Motorola Mobility (2012, later sold to Lenovo):** To gain access to patents.\n    *   **DeepMind (2014):** An artificial intelligence company.\n*   **Product Development:** Google launched numerous new products and services, including:\n    *   **Gmail (2004):** Email service.\n    *   **Google Maps (2005):** Mapping and navigation service.\n    *   **Google Chrome (2008):** Web browser.\n    *   **Google Drive (2012):** Cloud storage service.\n    *   **Google Cloud Platform (GCP):** Cloud computing services.\n    *   **Google Assistant:** Voice assistant.\n    *   **Pixel Phones:** Google-branded smartphones.\n*   **Alphabet Inc. (2015):** Google restructured and created a parent company called Alphabet Inc. This allowed for better organization and management of Google's diverse businesses and investments, including more experimental projects like Waymo (self-driving cars) and Verily (life sciences).  Sundar Pichai became the CEO of Google.\n\n**Key Technological Innovations:**\n\n*   **PageRank:** Revolutionized search engine ranking.\n*   **Distributed Computing:** Google built massive data centers and developed distributed computing technologies to handle the enormous amount of data it processes.\n*   **Artificial Intelligence and Machine Learning:** Google has become a leader in AI and machine learning, applying these technologies to many of its products and services, including search, translation, image recognition, and speech recognition.\n*   **Cloud Computing:** Google Cloud Platform provides a range of cloud computing services to businesses and developers.\n\n**Impact and Influence:**\n\n*   **Dominant Search Engine:** Google is the dominant search engine globally, processing billions of searches every day.\n*   **Advertising Powerhouse:** Google is one of the largest advertising companies in the world, generating significant revenue from online advertising.\n*   **Mobile Operating System:** Android is the most widely used mobile operating system in the world.\n*   **Innovation Driver:** Google has been a major driver of innovation in areas such as AI, cloud computing, and autonomous vehicles.\n*   **Cultural Influence:** Google has become a ubiquitous part of modern life, influencing how people access information, communicate, and interact with technology.\n\n**Controversies and Challenges:**\n\n*   **Privacy Concerns:** Google has faced criticism regarding its collection and use of user data, raising privacy concerns.\n*   **Antitrust Concerns:** Google has been subject to antitrust investigations and lawsuits in various countries, alleging anti-competitive practices.\n*   **Fake News and Misinformation:** Google has struggled with the spread of fake news and misinformation on its platforms.\n*   **Ethical AI:**  Ethical concerns surround the development and use of AI, particularly regarding bias, fairness, and accountability.\n\n**In summary, Google has grown from a small research project into one of the world's most powerful and influential companies. Its innovations have transformed the way people access information, communicate, and interact with technology. While it faces challenges related to privacy, antitrust, and ethical AI, Google continues to be a major force in the technology industry.**\n\nWhat specific aspects of Google are you interested in?  Perhaps:\n\n*   Google's search algorithm\n*   Google's advertising business\n*   Android operating system\n*   Google's AI efforts\n*   Google's ethical challenges\n*   The founders, Larry Page and Sergey Brin\n*   Specific Google products like Google Maps or Gmail.\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"response = chat.send_message('Do you remember what is my name is ?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:59.093083Z","iopub.execute_input":"2025-11-22T07:48:59.093367Z","iopub.status.idle":"2025-11-22T07:48:59.669746Z","shell.execute_reply.started":"2025-11-22T07:48:59.093345Z","shell.execute_reply":"2025-11-22T07:48:59.668866Z"}},"outputs":[{"name":"stdout","text":"Yes, your name is Akmal Fikry.\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"\n**Choose a model**\n\nThe Gemini API provides access to a number of models from the Gemini model family.","metadata":{}},{"cell_type":"code","source":"for model in client.models.list():\n    print(model.name) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:59.670621Z","iopub.execute_input":"2025-11-22T07:48:59.670841Z","iopub.status.idle":"2025-11-22T07:48:59.857751Z","shell.execute_reply.started":"2025-11-22T07:48:59.670824Z","shell.execute_reply":"2025-11-22T07:48:59.856963Z"}},"outputs":[{"name":"stdout","text":"models/embedding-gecko-001\nmodels/gemini-2.5-pro-preview-03-25\nmodels/gemini-2.5-flash\nmodels/gemini-2.5-pro-preview-05-06\nmodels/gemini-2.5-pro-preview-06-05\nmodels/gemini-2.5-pro\nmodels/gemini-2.0-flash-exp\nmodels/gemini-2.0-flash\nmodels/gemini-2.0-flash-001\nmodels/gemini-2.0-flash-exp-image-generation\nmodels/gemini-2.0-flash-lite-001\nmodels/gemini-2.0-flash-lite\nmodels/gemini-2.0-flash-lite-preview-02-05\nmodels/gemini-2.0-flash-lite-preview\nmodels/gemini-2.0-pro-exp\nmodels/gemini-2.0-pro-exp-02-05\nmodels/gemini-exp-1206\nmodels/gemini-2.0-flash-thinking-exp-01-21\nmodels/gemini-2.0-flash-thinking-exp\nmodels/gemini-2.0-flash-thinking-exp-1219\nmodels/gemini-2.5-flash-preview-tts\nmodels/gemini-2.5-pro-preview-tts\nmodels/learnlm-2.0-flash-experimental\nmodels/gemma-3-1b-it\nmodels/gemma-3-4b-it\nmodels/gemma-3-12b-it\nmodels/gemma-3-27b-it\nmodels/gemma-3n-e4b-it\nmodels/gemma-3n-e2b-it\nmodels/gemini-flash-latest\nmodels/gemini-flash-lite-latest\nmodels/gemini-pro-latest\nmodels/gemini-2.5-flash-lite\nmodels/gemini-2.5-flash-image-preview\nmodels/gemini-2.5-flash-image\nmodels/gemini-2.5-flash-preview-09-2025\nmodels/gemini-2.5-flash-lite-preview-09-2025\nmodels/gemini-3-pro-preview\nmodels/gemini-3-pro-image-preview\nmodels/nano-banana-pro-preview\nmodels/gemini-robotics-er-1.5-preview\nmodels/gemini-2.5-computer-use-preview-10-2025\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\nmodels/gemini-embedding-001\nmodels/aqa\nmodels/imagen-4.0-generate-preview-06-06\nmodels/imagen-4.0-ultra-generate-preview-06-06\nmodels/imagen-4.0-generate-001\nmodels/imagen-4.0-ultra-generate-001\nmodels/imagen-4.0-fast-generate-001\nmodels/veo-2.0-generate-001\nmodels/veo-3.0-generate-001\nmodels/veo-3.0-fast-generate-001\nmodels/veo-3.1-generate-preview\nmodels/veo-3.1-fast-generate-preview\nmodels/gemini-2.0-flash-live-001\nmodels/gemini-live-2.5-flash-preview\nmodels/gemini-2.5-flash-live-preview\nmodels/gemini-2.5-flash-native-audio-latest\nmodels/gemini-2.5-flash-native-audio-preview-09-2025\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"**The models.list response also returns additional information about the model's capabilities, like the token limits and supported parameters.**\n\n","metadata":{}},{"cell_type":"code","source":"from pprint import pprint\nfor model in client.models.list():\n    if model.name == 'models/gemini-2.0-flash':\n        pprint(model.to_json_dict())\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:59.858635Z","iopub.execute_input":"2025-11-22T07:48:59.858903Z","iopub.status.idle":"2025-11-22T07:48:59.911708Z","shell.execute_reply.started":"2025-11-22T07:48:59.858883Z","shell.execute_reply":"2025-11-22T07:48:59.910939Z"}},"outputs":[{"name":"stdout","text":"{'description': 'Gemini 2.0 Flash',\n 'display_name': 'Gemini 2.0 Flash',\n 'input_token_limit': 1048576,\n 'name': 'models/gemini-2.0-flash',\n 'output_token_limit': 8192,\n 'supported_actions': ['generateContent',\n                       'countTokens',\n                       'createCachedContent',\n                       'batchGenerateContent'],\n 'tuned_model_info': {},\n 'version': '2.0'}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from pprint import pprint\nfor model in client.models.list():\n    if model.name == 'models/embedding-gecko-001':\n        pprint(model.to_json_dict())\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:48:59.912534Z","iopub.execute_input":"2025-11-22T07:48:59.912763Z","iopub.status.idle":"2025-11-22T07:49:00.102272Z","shell.execute_reply.started":"2025-11-22T07:48:59.912743Z","shell.execute_reply":"2025-11-22T07:49:00.101327Z"}},"outputs":[{"name":"stdout","text":"{'description': 'Obtain a distributed representation of a text.',\n 'display_name': 'Embedding Gecko',\n 'input_token_limit': 1024,\n 'name': 'models/embedding-gecko-001',\n 'output_token_limit': 1,\n 'supported_actions': ['embedText', 'countTextTokens'],\n 'tuned_model_info': {},\n 'version': '001'}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"**generation parameters**\nOutput length\nWhen generating text with an LLM, the output length affects cost and performance. Generating more tokens increases computation, leading to higher energy consumption, latency, and cost.\n\nTo stop the model from generating tokens past a limit, you can specify **the max_output_tokens parameter** when using the **Gemini API**. Specifying this parameter does not influence the generation of the output tokens, s**o the output will not become more stylistically or textually succinct**, but it will stop generating tokens once the specified length is reached. Prompt engineering may be required to generate a more complete output for your given limit.","metadata":{}},{"cell_type":"code","source":"from google.genai import types\n\nshort_config = types.GenerateContentConfig(max_output_tokens=200)\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=short_config,\n    contents='Write a 1000 word essay on the importance of olives in modern society.')\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:49:00.103373Z","iopub.execute_input":"2025-11-22T07:49:00.103675Z","iopub.status.idle":"2025-11-22T07:49:01.791838Z","shell.execute_reply.started":"2025-11-22T07:49:00.103646Z","shell.execute_reply":"2025-11-22T07:49:01.791118Z"}},"outputs":[{"name":"stdout","text":"## The Enduring Olive: An Unassuming Pillar of Modern Society\n\nThe olive, that small, unassuming fruit, might seem an unlikely linchpin of modern society. Yet, its significance extends far beyond a simple garnish or a tasty addition to a Mediterranean meal. From its ancient roots to its contemporary applications, the olive, both as a fruit and as a source of oil, permeates our lives in ways both obvious and subtle, contributing to our health, economy, culture, and even our understanding of sustainability.\n\nOne of the most significant contributions of the olive lies in its profound impact on human health. Olives, particularly when consumed whole and unprocessed, are packed with beneficial compounds. They are a good source of vitamin E, a powerful antioxidant that protects cells from damage caused by free radicals. Monounsaturated fats, predominantly oleic acid, are abundant in olives and olive oil. These fats are known to lower \"bad\" cholesterol (LDL) levels while maintaining or increasing \"good\" cholesterol (\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"response = client.models.generate_content(\n    model = 'gemini-2.0-flash',\n    config = short_config,\n    contents = 'Write a short poem on the importance of olives in modern society.'\n)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:49:01.792804Z","iopub.execute_input":"2025-11-22T07:49:01.793113Z","iopub.status.idle":"2025-11-22T07:49:03.012386Z","shell.execute_reply.started":"2025-11-22T07:49:01.793085Z","shell.execute_reply":"2025-11-22T07:49:03.011533Z"}},"outputs":[{"name":"stdout","text":"From ancient groves to modern plate,\nThe olive shines, a culinary fate.\nIn oil it flows, a healthy grace,\nOn salads bright, it finds its place.\n\nIn tapenades, a savory blend,\nA briny gift, a faithful friend.\nFrom pizza's crust to cocktails cool,\nThe humble olive, breaks all rule,\n\nOf simple taste, it elevates,\nA tiny fruit that seals our fates,\nWith flavors rich and history deep,\nThe olive's role, we vow to keep.\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"**Temperature**\n\nTemperature controls the degree of randomness in token selection. **Higher temperatures** result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while **lower temperatures** have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.","metadata":{}},{"cell_type":"code","source":"high_temp_config = types.GenerateContentConfig(temperature=2.0)\n\nfor _ in range(5):\n    respomse = client.models.generate_content(\n        model = 'gemini-2.0-flash',\n      config=high_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\nif response.text:\n    print(response.text,'-'* 25)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:49:03.013266Z","iopub.execute_input":"2025-11-22T07:49:03.013476Z","iopub.status.idle":"2025-11-22T07:49:05.626160Z","shell.execute_reply.started":"2025-11-22T07:49:03.013460Z","shell.execute_reply":"2025-11-22T07:49:05.625182Z"}},"outputs":[{"name":"stdout","text":"From ancient groves to modern plate,\nThe olive shines, a culinary fate.\nIn oil it flows, a healthy grace,\nOn salads bright, it finds its place.\n\nIn tapenades, a savory blend,\nA briny gift, a faithful friend.\nFrom pizza's crust to cocktails cool,\nThe humble olive, breaks all rule,\n\nOf simple taste, it elevates,\nA tiny fruit that seals our fates,\nWith flavors rich and history deep,\nThe olive's role, we vow to keep.\n -------------------------\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"low_temp_config = types.GenerateContentConfig(temperature=0.0)\n\nfor _ in range(5):\n    respomse = client.models.generate_content(\n        model = 'gemini-2.0-flash',\n      config=low_temp_config,\n      contents='Pick a random colour... (respond in a single word)')\n\nif response.text:\n    print(response.text,'-'*25) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:49:05.627200Z","iopub.execute_input":"2025-11-22T07:49:05.627507Z","iopub.status.idle":"2025-11-22T07:50:26.492462Z","shell.execute_reply.started":"2025-11-22T07:49:05.627481Z","shell.execute_reply":"2025-11-22T07:50:26.491664Z"}},"outputs":[{"name":"stdout","text":"From ancient groves to modern plate,\nThe olive shines, a culinary fate.\nIn oil it flows, a healthy grace,\nOn salads bright, it finds its place.\n\nIn tapenades, a savory blend,\nA briny gift, a faithful friend.\nFrom pizza's crust to cocktails cool,\nThe humble olive, breaks all rule,\n\nOf simple taste, it elevates,\nA tiny fruit that seals our fates,\nWith flavors rich and history deep,\nThe olive's role, we vow to keep.\n -------------------------\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**Top-P**\n\nTop-P parameter is also used to control the diversity of the model's output.\n\nTop-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n","metadata":{}},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    # these are the default values for gemini-2.0-flash \n    temperature = 1.0,\n    top_p = 0.95,\n)\nstory_prompt = \"You are a creative writer.Write a short story about a person who goes on an adventure.\"\nresponse = client.models.generate_content(\n    model = 'gemini-2.0-flash',\n    config=model_config,\n    contents=story_prompt\n)\nprint(response.text)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:50:26.493421Z","iopub.execute_input":"2025-11-22T07:50:26.493655Z","iopub.status.idle":"2025-11-22T07:50:33.383068Z","shell.execute_reply.started":"2025-11-22T07:50:26.493635Z","shell.execute_reply":"2025-11-22T07:50:33.382209Z"}},"outputs":[{"name":"stdout","text":"Elara was a mapmaker of dust. Not grand, sweeping landscapes, mind you. Her maps documented the subtle undulations of dust bunnies under furniture, the swirling currents of lint in forgotten corners, the topography of crumbs clinging to the sofa cushions. It was a lonely, meticulous existence, punctuated only by the wheeze of her ancient vacuum cleaner, affectionately nicknamed “The Lung.”\n\nElara yearned for something more than the microscopic. She dreamt of real adventures, of soaring mountains and raging rivers, not miniature Everest’s and the treacherous Trench of Crumbs.\n\nOne day, while charting the unexplored territory behind the radiator, she unearthed something extraordinary. Not a dust bunny, nor a stray sock, but a tiny, intricately carved wooden box. It was no bigger than her thumb, adorned with symbols she couldn't decipher. Inside, nestled on a bed of faded velvet, was a single, shimmering blue feather.\n\nThe feather pulsed with a faint warmth. As Elara held it, a jolt of electricity coursed through her, filling her with an overwhelming desire to… fly. Not literally, of course. But to *go*.\n\nThat night, she slept fitfully, dreaming of turquoise skies and emerald landscapes. When she woke, the blue feather was glowing faintly, pointing towards the window. On the windowsill, a single, dew-kissed leaf lay – a type she’d never seen before, not in any textbook, not on any map.\n\nThe leaf was an invitation.\n\nElara, the mapmaker of dust, packed a bag with essentials: her magnifying glass (for close inspection, naturally), a notebook, her most reliable pen, a thermos of lukewarm tea, and the tiny wooden box.\n\nFollowing the direction indicated by the blue feather, she left her dusty apartment, a place she'd called home for far too long.\n\nHer adventure began in the alleyway behind her building. The feather led her to a crack in the brick wall, normally dismissed as a flaw. This time, however, the crack shimmered with an otherworldly light. Elara hesitated only for a moment before squeezing through.\n\nOn the other side was not another alley, but a hidden garden, bathed in the soft glow of bioluminescent fungi. Giant, shimmering flowers bloomed in vibrant colors she couldn’t name. The air hummed with the chirping of unseen insects. It was a world existing right beside her own, invisible to the unseeing eye.\n\nThe feather guided her deeper into the garden. She crossed bridges made of interwoven vines, navigated tunnels carved through the roots of ancient trees, and marveled at waterfalls that cascaded into pools of liquid moonlight. She met strange and wonderful creatures – gossiping snails with iridescent shells, fireflies that whispered secrets in the wind, and owls with eyes like molten gold.\n\nElara, accustomed to the silent scrutiny of dust bunnies, felt alive.\n\nThe journey wasn't without its challenges. There were thorny thickets to navigate, precarious ledges to cross, and riddles posed by mischievous sprites to solve. But Elara, the mapmaker, discovered a hidden strength within herself. Her meticulous observation skills, honed in the dusty corners of her apartment, were now her greatest asset.\n\nFinally, after days of wandering, the feather led her to the heart of the garden: a towering, ancient oak tree. Carved into its trunk was a single word, illuminated by the feather's glow: \"BELIEF.\"\n\nElara understood. The adventure wasn’t about the place, it was about the belief in the possibility of something more. She'd found a world hidden in plain sight, not by chance, but because she finally chose to see it.\n\nAs she touched the word, the feather dissolved into a shower of shimmering dust. The tiny wooden box clicked open, revealing a miniature map, no bigger than her fingernail. It depicted the very garden she stood in, complete with its hidden pathways and fantastical creatures.\n\nElara smiled. Her days of mapping dust were over. She had a whole new world to explore, and this time, she had a real map to guide her. The adventure, she realized, had only just begun.\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"**Prompting**\n\nThis section contains some prompts from the chapter for you to try out directly in the API. Try changing the text here to see how each prompt performs with different instructions.","metadata":{}},{"cell_type":"markdown","source":"**Zero-shot**\n\nZero-shot prompts are prompts that describe the request for the model directly.","metadata":{}},{"cell_type":"markdown","source":"User :> \n\nClassify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment:\n\n\nModel :> \n\nSentiment: POSITIVE\nThe review praises the movie as a \"masterpiece\" and expresses a wish for more films like it. While it mentions the movie being \"disturbing,\" this is presented within the context of a positive overall assessment.","metadata":{}},{"cell_type":"code","source":"model_config = types.GenerateContentConfig(\n    temperature=0.1,\n    top_p=1,\n    max_output_tokens=5,\n)\n\nzero_shot_prompt = \"\"\" Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=model_config,\n    contents=zero_shot_prompt\n)\nprint(response.text)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:50:33.384006Z","iopub.execute_input":"2025-11-22T07:50:33.384432Z","iopub.status.idle":"2025-11-22T07:50:34.106699Z","shell.execute_reply.started":"2025-11-22T07:50:33.384404Z","shell.execute_reply":"2025-11-22T07:50:34.105947Z"}},"outputs":[{"name":"stdout","text":"POSITIVE\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"Enum mode\n\nThe models are trained to generate text, and while the Gemini 2.0 models are great at following instructions, other models can sometimes produce more text than you may wish for.\n\nThe Gemini API has an **Enum mode** feature that allows you to constrain the output to a fixed set of values.","metadata":{}},{"cell_type":"code","source":"import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE  = \"positive\"\n    NETURAL = \"netural\"\n    NEGATIVE = \"negative\"\n\nresponce = client.models.generate_content(\n    model= 'gemini-2.0-flash',\n    config= types.GenerateContentConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment),\n    \n    contents=zero_shot_prompt)\n\nprint(response.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:50:34.107581Z","iopub.execute_input":"2025-11-22T07:50:34.108491Z","iopub.status.idle":"2025-11-22T07:50:34.726566Z","shell.execute_reply.started":"2025-11-22T07:50:34.108466Z","shell.execute_reply":"2025-11-22T07:50:34.725712Z"}},"outputs":[{"name":"stdout","text":"POSITIVE\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"When using constrained output like an enum, like python SDk willl attempt ot convert the model's text response into a python object automatically. its stored in the response.parsed field.","metadata":{}},{"cell_type":"code","source":"enum_response = response.parsed\nprint(enum_response)\nprint(type(enum_response))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:50:34.727418Z","iopub.execute_input":"2025-11-22T07:50:34.728120Z","iopub.status.idle":"2025-11-22T07:50:34.732486Z","shell.execute_reply.started":"2025-11-22T07:50:34.728093Z","shell.execute_reply":"2025-11-22T07:50:34.731809Z"}},"outputs":[{"name":"stdout","text":"None\n<class 'NoneType'>\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"**One- Shot and few-Shot**\n\nproviding an exmaple of the expected response is known as a \"one-shot\" prompt. when you provide multiple examples,its a \"Few-shot\" prompt \n","metadata":{}},{"cell_type":"code","source":"few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n```\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ),\n    contents=[few_shot_prompt, customer_order])\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:50:34.733423Z","iopub.execute_input":"2025-11-22T07:50:34.733656Z","iopub.status.idle":"2025-11-22T07:50:35.425927Z","shell.execute_reply.started":"2025-11-22T07:50:34.733628Z","shell.execute_reply":"2025-11-22T07:50:35.424855Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"pineapple\"]\n}\n```\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"**JSON mode**\n\nTO Provide control over the schema, and to ensure that you only receive JSON(with no other text or markdown)","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ),\n    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:50:35.426871Z","iopub.execute_input":"2025-11-22T07:50:35.427175Z","iopub.status.idle":"2025-11-22T07:50:36.047163Z","shell.execute_reply.started":"2025-11-22T07:50:35.427152Z","shell.execute_reply":"2025-11-22T07:50:36.046198Z"}},"outputs":[{"name":"stdout","text":"{\n  \"size\": \"large\",\n  \"ingredients\": [\"apple\", \"chocolate\"],\n  \"type\": \"dessert\"\n}\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"**Chain of Thought (CoT)**\n\nDirect prompting on LLMs can return answers quickly and efficiently, but they can be prone to hallucination. The answer may \"look\" correct but is incorrect in terms of factuality and reasoning.\nChain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n\nModels like the Gemini family are trained to be \"chatty\" or \"thoughtful\" and will provide reasoning steps without prompting, so for this simple example you can ask the model to be more direct in the prompt to force a non-reasoning response. Try re-running this step if the model gets lucky and gets the answer correct on the first try.","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"When i was 4 years old, my partner was 3 times my age. Now, i am 20 years old. How old is my partner? Return the answer directly\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\n\nMarkdown(response.text)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:50:36.047937Z","iopub.execute_input":"2025-11-22T07:50:36.048227Z","iopub.status.idle":"2025-11-22T07:50:36.600710Z","shell.execute_reply.started":"2025-11-22T07:50:36.048201Z","shell.execute_reply":"2025-11-22T07:50:36.599991Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"52\n"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=prompt)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:50:36.601592Z","iopub.execute_input":"2025-11-22T07:50:36.601855Z","iopub.status.idle":"2025-11-22T07:50:37.080509Z","shell.execute_reply.started":"2025-11-22T07:50:36.601835Z","shell.execute_reply":"2025-11-22T07:50:37.079647Z"}},"outputs":[{"name":"stdout","text":"52\n\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"**ReAct: Reason and act**\n\nIn this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the \"Prompting\" whitepaper.","metadata":{}},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"\n\n# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:50:37.083984Z","iopub.execute_input":"2025-11-22T07:50:37.084275Z","iopub.status.idle":"2025-11-22T07:50:37.090269Z","shell.execute_reply.started":"2025-11-22T07:50:37.084254Z","shell.execute_reply":"2025-11-22T07:50:37.089101Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\n# You will perform the Action; so generate up to, but not including, the Observation.\nreact_config = types.GenerateContentConfig(\n    stop_sequences=[\"\\nObservation\"],\n    system_instruction=model_instructions + example1 + example2,\n)\n\n# Create a chat that has the model instructions and examples pre-seeded.\nreact_chat = client.chats.create(\n    model='gemini-2.0-flash',\n    config=react_config,\n)\n\nresp = react_chat.send_message(question)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:50:37.091128Z","iopub.execute_input":"2025-11-22T07:50:37.091425Z","iopub.status.idle":"2025-11-22T07:50:37.801534Z","shell.execute_reply.started":"2025-11-22T07:50:37.091400Z","shell.execute_reply":"2025-11-22T07:50:37.800583Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI need to find the \"transformers NLP paper\" and then find the youngest author listed on the paper.\n\nAction 1\n<search>transformers NLP paper</search>\n\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T07:50:37.802447Z","iopub.execute_input":"2025-11-22T07:50:37.802672Z","iopub.status.idle":"2025-11-22T07:50:38.765227Z","shell.execute_reply.started":"2025-11-22T07:50:37.802655Z","shell.execute_reply":"2025-11-22T07:50:38.764458Z"}},"outputs":[{"name":"stdout","text":"Thought 2\nNow I have the list of authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. I need to find the youngest one. I will search each of them on wikipedia and find their birth date, then find the youngest one.\n\nAction 2\n<search>Ashish Vaswani</search>\n\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"**Thinking mode**\n\nThe experiemental Gemini Flash 2.0 **\"Thinking\"** model has been trained to generate the **\"thinking process\"** the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses.\n\nUsing a **\"thinking mode\"** model can provide you with *high-quality responses without needing specialised prompting like the previous approaches*. One reason this technique is effective is that you induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated.","metadata":{}},{"cell_type":"code","source":"import io\nfrom IPython.display import Markdown, clear_output\n\n\nresponse = client.models.generate_content_stream(\n    model='gemini-2.0-flash-thinking-exp',\n    contents='Who was the youngest author listed on the transformers NLP paper?',\n)\n\nbuf = io.StringIO()\nfor chunk in response:\n    buf.write(chunk.text)\n    # Display the response as it is streamed\n    print(chunk.text, end='')\n\n# And then render the finished response as formatted markdown.\nclear_output()\nMarkdown(buf.getvalue())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T09:14:33.505792Z","iopub.execute_input":"2025-11-22T09:14:33.506558Z","iopub.status.idle":"2025-11-22T09:14:33.723396Z","shell.execute_reply.started":"2025-11-22T09:14:33.506530Z","shell.execute_reply":"2025-11-22T09:14:33.722169Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2739997184.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Display the response as it is streamed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content_stream\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5505\u001b[0m         \u001b[0;31m# Then get function response parts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5506\u001b[0m         \u001b[0;31m# Yield chunks only if there's no function response parts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5507\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5508\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfunction_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5509\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content_stream\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4407\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4409\u001b[0;31m     for response_dict in self._api_client.request_streamed(\n\u001b[0m\u001b[1;32m   4410\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4411\u001b[0m     ):\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest_streamed\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    657\u001b[0m     )\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m     \u001b[0msession_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msession_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    555\u001b[0m       )\n\u001b[1;32m    556\u001b[0m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_httpx_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttpx_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m       return HttpResponse(\n\u001b[1;32m    559\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mstatus_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mClientError\u001b[0m: 429 Too Many Requests. {'message': 'Response not read', 'status': 'Too Many Requests'}"],"ename":"ClientError","evalue":"429 Too Many Requests. {'message': 'Response not read', 'status': 'Too Many Requests'}","output_type":"error"}],"execution_count":47},{"cell_type":"markdown","source":"**Code prompting**\n\nThe Gemini family of models can be used to generate code, configuration and scripts. Generating code can be helpful when learning to code, learning a new language or for rapidly generating a first draft.\n\n\nIt's important to be aware that since LLMs can make mistakes, and can repeat training data, it's essential to read and test your code first, and comply with any relevant licenses.","metadata":{}},{"cell_type":"code","source":"\n\ncode_prompt = \"\"\"\nWrite a python function to calculate the factorial of a number.\nNo explantion, provide only the code.\"\"\"\n\nresponce = client.models.generate_content(\n    model = 'gemini-2.0-flash',\n    config=types.GenerateContentConfig(\n        temperature = 1,\n        top_p = 1,\n        max_output_tokens=1024,\n    ),\n    contents = code_prompt)\n\nMarkdown(response.text)\n\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T08:00:18.509273Z","iopub.execute_input":"2025-11-22T08:00:18.509591Z","iopub.status.idle":"2025-11-22T08:00:19.180541Z","shell.execute_reply.started":"2025-11-22T08:00:18.509566Z","shell.execute_reply":"2025-11-22T08:00:19.179743Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n```"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"from pprint import pprint\nconfig = types.GenerateContentConfig(\n    tools = [types.Tool(code_execution=types.ToolCodeExecution())],\n    \n)\n\ncode_exec_prompt = \"\"\" \nGenerate the first 14 odd prime numbers, then calculate their sum.\n\"\"\"\nresponse = client.models.generate_content(\n    model = 'gemini-2.0-flash',\n    config=config,\n    contents=code_exec_prompt)\n\n\nfor part in response.candidates[0].content.parts:\n    pprint(part.to_json_dict())\n    print(\"------\")\n\n\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T08:06:12.743550Z","iopub.execute_input":"2025-11-22T08:06:12.744395Z","iopub.status.idle":"2025-11-22T08:06:14.979645Z","shell.execute_reply.started":"2025-11-22T08:06:12.744364Z","shell.execute_reply":"2025-11-22T08:06:14.978843Z"}},"outputs":[{"name":"stdout","text":"{'text': \"Okay, I can do that. First, I'll generate the first 14 odd prime \"\n         'numbers. Remember that a prime number is a number greater than 1 '\n         'that has only two factors: 1 and itself. The first few prime numbers '\n         'are 2, 3, 5, 7, 11, and so on. Since the question asks for *odd* '\n         \"prime numbers, I'll start with 3, excluding 2.\\n\"\n         '\\n'\n         'Here are the first 14 odd prime numbers: 3, 5, 7, 11, 13, 17, 19, '\n         '23, 29, 31, 37, 41, 43, 47\\n'\n         '\\n'\n         'Now, I will calculate their sum using a python code block.\\n'\n         '\\n'}\n------\n{'executable_code': {'code': 'numbers = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, '\n                             '37, 41, 43, 47]\\n'\n                             'total = sum(numbers)\\n'\n                             'print(total)\\n',\n                     'language': 'PYTHON'}}\n------\n{'code_execution_result': {'outcome': 'OUTCOME_OK', 'output': '326\\n'}}\n------\n{'text': 'The sum of the first 14 odd prime numbers is 326.\\n'}\n------\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nresponse = client.models.generate_content(\n    model='gemini-2.0-flash',\n    contents=explain_prompt)\n\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T09:01:35.189271Z","iopub.execute_input":"2025-11-22T09:01:35.189575Z","iopub.status.idle":"2025-11-22T09:01:39.159273Z","shell.execute_reply.started":"2025-11-22T09:01:35.189554Z","shell.execute_reply":"2025-11-22T09:01:39.158316Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file is a script, primarily designed to enhance your command-line prompt with Git repository information.  It's commonly known as `bash-git-prompt`.\n\nHere's a high-level breakdown:\n\n*   **What it does:** It modifies your shell prompt (the text you see before you type a command) to display information about the Git repository you're currently in. This includes things like the current branch, whether there are uncommitted changes, if your local branch is ahead or behind the remote branch, and the last command state.\n\n*   **Why you'd use it:**\n\n    *   **Quick Git Status:**  At a glance, you can see the state of your Git repository without having to run `git status`. This saves you time and keystrokes.\n\n    *   **Improved Workflow:**  The visual cues help you keep track of your Git workflow, making it easier to remember what branch you're on, if you have changes to commit, etc.\n\n    *   **Customization:**  The script is highly customizable. You can change the colors, symbols, and information displayed to suit your preferences.\n\n    *   **Cross-Shell Compatibility:**  It aims to work with both Bash and Zsh shells.\n\nIn essence, it's a productivity tool for developers who use Git, making it easier to manage their repositories from the command line. You would typically \"source\" this script in your `.bashrc` or `.zshrc` file so that it's loaded every time you open a new terminal.\n"},"metadata":{}}],"execution_count":43},{"cell_type":"markdown","source":"Check out the whitepaper issued with today's content,\nTry out the apps listed at the top of this notebook ([TextFX, SQL Talk](http://) and[ NotebookLM](http://)),\nRead the [Introduction to Prompting](http://) from the Gemini API docs,\nExplore the Gemini API's [prompt gallery](http://) and try them out in AI Studio,\nCheck out the Gemini API cookbook for [inspirational examples](http://) and [educational quickstarts](http://).","metadata":{}}]}